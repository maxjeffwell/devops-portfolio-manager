# Velero Backup Schedules
veleroSchedules:
  enabled: true
  namespace: velero
  excludedNamespaces:
    - gpu-operator
  daily:
    schedule: "0 2 * * *"
    ttl: 168h0m0s
  weekly:
    schedule: "0 3 * * 0"
    ttl: 720h0m0s

# Velero Backup Alerts
veleroAlerts:
  enabled: true
  namespace: velero

# Velero Backup Verification (weekly restore test)
veleroBackupVerify:
  enabled: true
  # Run weekly on Sundays at 4 AM UTC
  schedule: "0 4 * * 0"

# Gotify Bridge (AlertManager → Gotify notification relay)
gotifyBridge:
  enabled: true
  image:
    repository: ghcr.io/druggeri/alertmanager_gotify_bridge
    tag: "2.3.2"
  gotifyEndpoint: "http://gotify.monitoring.svc.cluster.local"
  tokenSecret:
    name: gotify-alertmanager-token
    key: gotify-token
  defaultPriority: "5"
  resources:
    requests:
      cpu: 10m
      memory: 16Mi
    limits:
      cpu: 50m
      memory: 64Mi
  serviceMonitor:
    enabled: true

# Custom Ingress Configuration (separate from kube-prometheus-stack ingress)
ingress:
  enabled: true
  grafanaHost: grafana.el-jefe.me
  grafanaService: prometheus-grafana
  grafanaPort: 80
  prometheusHost: prometheus.el-jefe.me
  prometheusService: prometheus-kube-prometheus-prometheus
  prometheusPort: 9090

# Kube-Prometheus-Stack Configuration
kube-prometheus-stack:
  # Prometheus Configuration
  prometheus:
    prometheusSpec:
      # Number of replicas
      replicas: 1
      # Retention period for metrics
      retention: 15d
      # Node selector - run on VPS to ensure network access to all targets
      nodeSelector:
        kubernetes.io/hostname: vmi2951245
      # Storage configuration
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: local-path
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 50Gi
      # Resource limits
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          cpu: 1000m
          memory: 4Gi
      # Remote write to Mimir for long-term metric storage
      remoteWrite:
        - url: http://prometheus-mimir-gateway.monitoring.svc.cluster.local/api/v1/push
          name: mimir
          remoteTimeout: 30s
          writeRelabelConfigs:
            # Drop high-cardinality apiserver request/watch metrics
            - sourceLabels: [__name__]
              regex: "apiserver_request_duration_seconds_bucket|apiserver_request_body_size_bytes_bucket|apiserver_response_body_size_bytes_bucket|apiserver_watch_events_sizes_bucket"
              action: drop
      # Service monitors for scraping application metrics
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false

    # Disable kube-prometheus-stack's built-in ingress (we use a custom Traefik ingress in templates/ingress.yaml)
    ingress:
      enabled: false

  # Grafana Configuration
  # ⚠️  SECURITY: Override adminPassword at deploy time!
  # helm install monitoring . --set kube-prometheus-stack.grafana.adminPassword="$(openssl rand -base64 24)"
  grafana:
    enabled: true
    adminPassword: "REPLACE_AT_DEPLOY_TIME"

    # Disable kube-prometheus-stack's built-in ingress (we use a custom Traefik ingress in templates/ingress.yaml)
    ingress:
      enabled: false

    # Configure Grafana for subdomain routing (grafana.el-jefe.me)
    grafana.ini:
      server:
        domain: grafana.el-jefe.me
        root_url: "https://grafana.el-jefe.me/"
        serve_from_sub_path: false


    # Additional data sources for Loki and Mimir
    additionalDataSources:
      - name: Loki
        type: loki
        uid: loki
        access: proxy
        url: http://prometheus-loki.monitoring.svc.cluster.local:3100
        editable: true
      - name: Mimir
        type: prometheus
        uid: mimir
        access: proxy
        url: http://prometheus-mimir-gateway.monitoring.svc.cluster.local/prometheus
        editable: true

    # Resource limits
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi

    # Default dashboards
    defaultDashboardsEnabled: true
    defaultDashboardsTimezone: UTC

  # AlertManager Configuration
  alertmanager:
    enabled: true
    config:
      global:
        resolve_timeout: 5m
      inhibit_rules:
        - equal: ['namespace', 'alertname']
          source_matchers:
            - severity = critical
          target_matchers:
            - severity =~ warning|info
        - equal: ['namespace', 'alertname']
          source_matchers:
            - severity = warning
          target_matchers:
            - severity = info
        - equal: ['namespace']
          source_matchers:
            - alertname = InfoInhibitor
          target_matchers:
            - severity = info
        - target_matchers:
            - alertname = InfoInhibitor
      receivers:
        - name: 'null'
        - name: 'gotify'
          webhook_configs:
            - url: 'http://alertmanager-gotify-bridge.monitoring.svc.cluster.local:8080/gotify_webhook'
              send_resolved: true
      route:
        group_by: ['namespace']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'null'
        routes:
          # Silence Watchdog (always-firing health check)
          - matchers:
              - alertname = "Watchdog"
            receiver: 'null'
          # Silence GPU operator DaemonSet false-positives only
          - matchers:
              - namespace = "gpu-operator"
              - alertname =~ "KubeDaemonSetMisScheduled|KubeDaemonSetRolloutStuck"
            receiver: 'null'
          # Silence k3s control plane alerts (bundled into k3s binary)
          - matchers:
              - alertname =~ "KubeControllerManagerDown|KubeSchedulerDown|KubeProxyDown"
            receiver: 'null'
          # Route critical alerts to Gotify
          - matchers:
              - severity = "critical"
            receiver: 'gotify'
          # Route warning alerts to Gotify
          - matchers:
              - severity = "warning"
            receiver: 'gotify'
    alertmanagerSpec:
      storage:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 2Gi
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 256Mi

    # Disable kube-prometheus-stack's built-in ingress (we use a custom Traefik ingress in templates/ingress.yaml)
    ingress:
      enabled: false

  # Node Exporter - collect node-level metrics
  nodeExporter:
    enabled: true

  # Kube State Metrics - collect Kubernetes object metrics
  kubeStateMetrics:
    enabled: true

  # Prometheus Operator
  prometheusOperator:
    enabled: true
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 256Mi

# MinIO - S3-compatible object storage for Loki and Mimir
minio:
  enabled: true
  mode: standalone
  replicas: 1
  existingSecret: minio-credentials
  persistence:
    enabled: true
    storageClass: local-path
    size: 20Gi
  nodeSelector:
    kubernetes.io/hostname: vmi2951245
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 1Gi
  # Auto-create buckets for Loki and Mimir
  buckets:
    - name: loki-chunks
      policy: none
      purge: false
    - name: loki-ruler
      policy: none
      purge: false
    - name: mimir-blocks
      policy: none
      purge: false
    - name: mimir-ruler
      policy: none
      purge: false
    - name: mimir-alertmanager
      policy: none
      purge: false
  metrics:
    serviceMonitor:
      enabled: true

# Loki - Log aggregation
loki:
  enabled: true
  deploymentMode: SingleBinary
  loki:
    auth_enabled: false
    schemaConfig:
      configs:
        - from: "2024-01-01"
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: loki_index_
            period: 24h
    storage:
      type: s3
      s3:
        endpoint: http://prometheus-minio.monitoring.svc.cluster.local:9000
        bucketnames: loki-chunks
        region: us-east-1
        access_key_id: ${rootUser}
        secret_access_key: ${rootPassword}
        insecure: true
        s3forcepathstyle: true
      bucketNames:
        chunks: loki-chunks
        ruler: loki-ruler
    limits_config:
      retention_period: 360h
    compactor:
      retention_enabled: true
      delete_request_store: filesystem
      working_directory: /var/loki/compactor
    rulerConfig:
      storage:
        type: s3
        s3:
          endpoint: http://prometheus-minio.monitoring.svc.cluster.local:9000
          bucketnames: loki-ruler
          region: us-east-1
          access_key_id: ${MINIO_ROOT_USER}
          secret_access_key: ${MINIO_ROOT_PASSWORD}
          insecure: true
          s3forcepathstyle: true
  singleBinary:
    replicas: 1
    nodeSelector:
      kubernetes.io/hostname: vmi2951245
    persistence:
      storageClass: local-path
      size: 5Gi
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 1000m
        memory: 1Gi
    extraEnvFrom:
      - secretRef:
          name: minio-credentials
    extraArgs:
      - -config.expand-env=true
  # Right-sized cache resources (chart defaults are 500m/9.8Gi and 500m/1.2Gi)
  chunksCache:
    allocatedMemory: 256
    resources:
      requests:
        cpu: 50m
        memory: 384Mi
      limits:
        cpu: 200m
        memory: 512Mi
  resultsCache:
    allocatedMemory: 128
    resources:
      requests:
        cpu: 50m
        memory: 192Mi
      limits:
        cpu: 200m
        memory: 256Mi
  # Disable components not needed in single-binary mode
  gateway:
    enabled: false
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0
  monitoring:
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
    lokiCanary:
      enabled: false

# Alloy - Log collector (DaemonSet on all nodes)
alloy:
  enabled: true
  alloy:
    configMap:
      create: false
      name: alloy-config
      key: config.alloy
    mounts:
      varlog: true
      dockercontainers: true
  controller:
    type: daemonset
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 256Mi

# Mimir - Long-term metric storage
mimir-distributed:
  rollout_operator:
    enabled: false
  enabled: true
  nameOverride: ""
  # Inject MinIO credentials into all Mimir pods for S3 config expansion
  global:
    extraEnvFrom:
      - secretRef:
          name: minio-credentials
  mimir:
    structuredConfig:
      common:
        storage:
          backend: s3
          s3:
            endpoint: prometheus-minio.monitoring.svc.cluster.local:9000
            access_key_id: ${rootUser}
            secret_access_key: ${rootPassword}
            insecure: true
      blocks_storage:
        s3:
          bucket_name: mimir-blocks
        tsdb:
          dir: /data/tsdb
        bucket_store:
          sync_dir: /data/tsdb-sync
      ruler_storage:
        s3:
          bucket_name: mimir-ruler
      alertmanager_storage:
        s3:
          bucket_name: mimir-alertmanager
      compactor:
        data_dir: /data/compactor
        compaction_interval: 30m
      ingester:
        ring:
          replication_factor: 1
      store_gateway:
        sharding_ring:
          replication_factor: 1
      alertmanager:
        sharding_ring:
          replication_factor: 1
      limits:
        # Single-tenant setup: generous limits to avoid rate limiting
        max_global_series_per_user: 500000
        ingestion_rate: 100000
        ingestion_burst_size: 2000000
        compactor_blocks_retention_period: 2160h
  # Single-node distributed deployment (1 replica each for essential components)
  ingester:
    replicas: 1
    nodeSelector:
      kubernetes.io/hostname: vmi2951245
    persistentVolume:
      storageClass: local-path
      size: 10Gi
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 2Gi
    zoneAwareReplication:
      enabled: false
  distributor:
    replicas: 1
    nodeSelector:
      kubernetes.io/hostname: vmi2951245
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 2Gi
  querier:
    replicas: 1
    nodeSelector:
      kubernetes.io/hostname: vmi2951245
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
  query_frontend:
    replicas: 1
    nodeSelector:
      kubernetes.io/hostname: vmi2951245
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 256Mi
  compactor:
    replicas: 1
    nodeSelector:
      kubernetes.io/hostname: vmi2951245
    persistentVolume:
      storageClass: local-path
      size: 10Gi
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
  store_gateway:
    replicas: 1
    nodeSelector:
      kubernetes.io/hostname: vmi2951245
    persistentVolume:
      storageClass: local-path
      size: 10Gi
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
    zoneAwareReplication:
      enabled: false
  # Non-essential components disabled for single-node
  query_scheduler:
    replicas: 0
  ruler:
    replicas: 0
  alertmanager:
    replicas: 0
  overrides_exporter:
    replicas: 0
  # Align Kafka message.max.bytes with Mimir producer expectation (~16MB)
  kafka:
    extraEnv:
      - name: KAFKA_MESSAGE_MAX_BYTES
        value: "16777216"
      - name: KAFKA_REPLICA_FETCH_MAX_BYTES
        value: "16777216"
    nodeSelector:
      kubernetes.io/hostname: vmi2951245
    podAnnotations:
      backup.velero.io/backup-volumes-excludes: kafka-data
  # Disable Mimir's built-in MinIO sub-chart (using shared MinIO)
  minio:
    enabled: false
